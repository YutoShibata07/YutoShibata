<html><head>
  <script type="text/javascript" async="" src="https://www.googletagmanager.com/gtag/js?id=G-QPJZ0J6NDH&amp;cx=c&amp;_slc=1"></script><script async="" src="//www.google-analytics.com/analytics.js"></script><script>
      (function (i, s, o, g, r, a, m) {
          i['GoogleAnalyticsObject'] = r;
          i[r] = i[r] || function () {
              (i[r].q = i[r].q || []).push(arguments)
          }, i[r].l = 1 * new Date();
          a = s.createElement(o),
              m = s.getElementsByTagName(o)[0];
          a.async = 1;
          a.src = g;
          m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

      ga('create', 'UA-108114467-1', 'auto');
      ga('send', 'pageview');
  </script>
  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link href="https://fonts.googleapis.com/css?family=Titillium Web" rel="stylesheet">
  <meta charset="utf-8">
  <style type="text/css">
      /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
      a {
          color: #1772d0;
          text-decoration: none;
      }

      a:focus, a:hover {
          color: #f09228;
          text-decoration: none;
      }

      body, td, th {
          font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
          font-size: 16px;
          font-weight: 400
      }

      heading {
          font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
          font-size: 19px;
          font-weight: 1000
      }

      strong {
          font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
          font-size: 16px;
          font-weight: 800
      }

      strongred {
          font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
          color: 'red';
          font-size: 16px
      }

      sectionheading {
          font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
          font-size: 22px;
          font-weight: 600
      }
  </style>
  <title>Yuto Shibata</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
  <script async="" defer="" src="https://buttons.github.io/buttons.js"></script>
</head>
<body data-new-gr-c-s-check-loaded="14.1201.0" data-gr-ext-installed="" class="vsc-initialized">
<table width="1000" border="0" align="center" cellspacing="0" cellpadding="20">
  <tbody><tr>
      <td halign="center">
          <p align="center">
              <font size="6">Yuto Shibata 柴田優斗</font>
          </p>
      </td>
  </tr>
  <tr>
      <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody><tr>
                  <td width="67%" valign="middle">
                      <p>I am a postdoctoral researcher at <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a>
                          hosted by <a href="https://profiles.stanford.edu/fei-fei-li">Prof. Fei-Fei Li</a>
                          and <a href="https://stanford.edu/~eadeli/">Prof. Ehsan Adeli</a>.
                          I received my PhD from <a href="https://www.utexas.edu/">UT Austin</a>
                          advised by <a href="http://www.cs.utexas.edu/users/grauman/">Prof. Kristen Grauman</a>.
                          I am broadly interested at building machine learning models that perceive the world with
                          multi-modalities and interact with the world.
                          Currently, I work on multimodal perception and generation for 3D scenes and humans.
                      </p>
                      <p>Previously,  I spent five months working with <a href="https://www.robots.ox.ac.uk/~vedaldi/">Prof. Andrea Vedaldi</a>
                          and <a href="https://nneverova.github.io/">Dr. Natalia Neverova</a> at FAIR, London.
                          I was a visiting researcher at <a href="https://research.fb.com/category/facebook-ai-research/">FAIR</a>
                          working with <a href="http://www.cs.utexas.edu/users/grauman/">Prof. Kristen Grauman</a> for two years.
                          In my undergrad, I spent a wonderful year working with <a href="http://www.cs.sfu.ca/~mori">Prof.
                          Greg Mori</a> on sports video analysis and efficient deep learning, eight months working with
                          <a href="http://web.stanford.edu/~alahi">Prof. Alexandre Alahi</a> on social navigation in
                          crowds, and eight months working with <a href="http://msavva.github.io">Prof. Manolis
                              Savva</a> on relational graph reasoning for navigation.</p>
                      <p>My first name is pronounced as /tʃæn'æn/ with the g being silent.</p>
                      <p><b>Research opportunities:</b> I am happy to collaborate with motivated undergrad and master students at Stanford.
                      I am also happy to answer questions about my research. If you are interested, please send me an email.</p>
                      <p align="center"><a>
                      </a><a href="files/cv.pdf">CV</a> | <a href="mailto:changanvr@gmail.com">E-Mail</a> | <a href="https://scholar.google.com/citations?hl=en&amp;user=9Uxf0ikAAAAJ">Google Scholar</a> |
                          <a href="https://github.com/ChanganVR">Github</a> |
                          <a href="https://twitter.com/changan_vr"> Twitter</a> |
                          <a href="files/dissertation.pdf">Dissertation</a>
                      </p>
                  </td>
                  <td width="100%" valign="top">
                      <img src="/images/YutoShibata.png" width="95%">
                  </td>
              </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody><tr>
                  <td>
                      <heading>News</heading>
                  </td>
              </tr>
          </tbody></table>
          <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
              <colgroup>
                  <col width="15%">
                  <col width="85%">
              </colgroup>
              <tbody>
              <tr>
                  <td valign="top" align="center"><strong>May 2024</strong></td>
                  <td>Joining <a href="https://svl.stanford.edu/">Stanford Vision and Learning Lab</a> as a postdoc researcher!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>May 2024</strong></td>
                  <td>I defended my PhD dissertation <a href="files/dissertation.pdf">4D Audio-Visual Learning: A Visual Perspective of Sound Propagation and Production</a>!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>March 2024</strong></td>
                  <td>We are organizing the first <a href="https://multimodalitiesfor3dscenes.github.io">Multimodalities for 3D Scenes (M3DS)</a> workshop at CVPR 2024!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>March 2024</strong></td>
                  <td>We are organizing the fifth <a href="https://embodied-ai.org/">Embodied AI</a> workshop at CVPR 2024!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>October 2023</strong></td>
                  <td>Organizing the second <a href="https://av4d.org/">AV4D</a> workshop at ICCV 2023!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>June 2023</strong></td>
                  <td>Giving one keynote talk at <a href="https://sites.google.com/view/ambientaiicassp2023/home">Ambient AI Workshop</a>, ICASSP23,
                      and one at <a href="https://sightsound.org/">Sight and Sound Workshop</a>, CVPR23
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>Feb 2023</strong></td>
                  <td>Co-organizing <a href="https://embodied-ai.org/">Embodied AI Workshop</a> and <a href="https://soundspaces.org/challenge">the 3rd SoundSpaces Challenge</a> at CVPR 2023!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>Jan 2023</strong></td>
                  <td>Co-organizing <a href="https://www.l3das.com/icassp2023/">L3DAS23: Learning 3D Audio Sources for Audio-Visual Extended Reality</a> at ICASSP 2023!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>October 2022</strong></td>
                  <td>We are organizing the first <a href="https://av4d.org">AV4D: Visual Learning of Sounds in Spaces</a> workshop at <a href="https://eccv2022.ecva.net/">ECCV 2022</a>!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>July 2022</strong></td>
                  <td>Joining <a href="https://ai.facebook.com/research/">FAIR London</a> for summer internship!
                  </td>
              </tr>
<!--                <tr>-->
<!--                    <td valign="top" align="center"><strong>July 2022</strong></td>-->
<!--                    <td>Receiving <a href="https://gradschool.utexas.edu/finances/travel-awards/professional-development-awards">Professional Development Awards</a> from UT Austin!-->
<!--                    </td>-->
<!--                </tr>-->
              <tr>
                  <td valign="top" align="center"><strong>March 2022</strong></td>
                  <td>I am very honored to receive the <a href="https://research.adobe.com/fellowship/previous-fellowship-award-winners/">2022 Adobe Research Fellowship</a>!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>Feb 2022</strong></td>
                  <td>Organizing the second <a href="https://soundspaces.org/challenge">SoundSpaces Challenge</a> at the
                      <a href="https://embodied-ai.org">Embodied AI Workshop</a>, CVPR 2022!
                  </td>
              </tr>
              <tr>
                  <td valign="top" align="center"><strong>Feb 2021</strong></td>
                  <td>Organizing the first <a href="https://soundspaces.org/challenge">SoundSpaces Challenge</a> at the
                      <a href="https://embodied-ai.org">Embodied AI Workshop</a>, CVPR 2021!
                  </td>
              </tr>
<!--                <tr>-->
<!--                    <td valign="top" align="center"><strong>July 2020</strong></td>-->
<!--                    <td>Released a curated reading list for embodied vision: <a href="https://github.com/ChanganVR/awesome-embodied-vision">awesome-embodied-vision</a>-->
<!--                    </td>-->
<!--                </tr>-->
              <tr>
                  <td valign="top" align="center"><strong>May 2020</strong></td>
                  <td>Joining <a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research</a>
                      as a visiting researcher
                  </td>
              </tr>
              </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody><tr>
                  <td width="100%" valign="middle">
                      <heading>Publications</heading>
                  </td>
              </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody><tr onmouseout="hoiswap_start()" onmouseover="hoiswap_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="hoiswap"><div class="vsc-controller"></div>
<!--                                <img src='images/action2sound.jpg' alt="sym" width="100%"-->
<!--                                                                    style="border-style: none">-->
                              <video width="100%" height="auto" autoplay="" loop="" muted="">
                                  <source src="images/hoiswap.mp4" type="video/mp4">
                              </video>
                          </div>
                      </div>
                      <script type="text/javascript">
                          function hoiswap_start() {
                              document.getElementById('hoiswap').style.opacity = "0.9";
                          }

                          function hoiswap_stop() {
                              document.getElementById('hoiswap').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="">
                          <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                          <heading>HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness</heading>
                      </a><br>Zihui Xue, Mi Luo, <strong>Changan Chen<strong>, Kristen Grauman<br>
                          <em>arXiv 2024<br></em>
                          <a href="https://arxiv.org/abs/2406.07754">paper | </a>
                          <a href="https://vision.cs.utexas.edu/projects/HOI-Swap/">project</a>
                  </strong></strong></p></td>
              </tr>

          </tbody></table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody><tr onmouseout="action2sound_start()" onmouseover="action2sound_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="action2sound" style="opacity: 0.9;"><img src="images/action2sound.jpg" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function action2sound_start() {
                              document.getElementById('action2sound').style.opacity = "0.9";
                          }

                          function action2sound_stop() {
                              document.getElementById('action2sound').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="">
                          <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                          <heading>Action2Sound: Ambient-Aware Generation of
                              Action Sounds from Egocentric Videos</heading>
                      </a><br><strong>Changan Chen*</strong>, Puyuan Peng*, Ami Baid, Sherry Xue, Wei-Ning Hsu, David Harwath, Kristen Grauman<br>
                          <em>ECCV 2024 <span style="color:red;">(<b>Oral</b>)</span><br></em>
                          <a href="https://arxiv.org/abs/2406.09272">paper | </a>
                          <a href="https://vision.cs.utexas.edu/projects/action2sound">project | </a>
                          <a href="https://ego4dsounds.github.io">data | </a>
                          <a href="https://github.com/ChanganVR/action2sound">code</a>
                  </p></td>
              </tr>

              <tr onmouseout="sim2real_start()" onmouseover="sim2real_stop()">
                  <td width="35%">
                      <div class="one">
                          <!-- <div class="two" id='sim2real'><img src='images/sim2real.png' alt="sym" width="80%"
                                                            style="border-style: none"></div> -->
                          <!-- center the image -->
                          <div class="two" id="sim2real" style="display: flex; justify-content: center; align-items: center;">
                              <img src="images/sim2real.png" alt="sym" width="80%" style="border-style: none">
                      </div>
                      <script type="text/javascript">
                          function sim2real_start() {
                              document.getElementById('sim2real').style.opacity = "0.9";
                          }

                          function sim2real_stop() {
                              document.getElementById('sim2real').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </div></td>
                  <td valign="top" width="65%">
                      <p><a href="">
                          <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                          <heading>Sim2Real Transfer for Audio-Visual Navigation with
                              Frequency-Adaptive Acoustic Field Prediction</heading>
                      </a><br><strong>Changan Chen*</strong>, Jordi Ramos*, Anshul Tomar*, Kristen Grauman<br>
                          <em>IROS 2024<br></em>
                          <a href="https://arxiv.org/pdf/2405.02821">paper |</a>
                          <a href="https://vision.cs.utexas.edu/projects/sim2real/">project</a>
                  </p></td>
              </tr>


              <tr onmouseout="activerir_start()" onmouseover="activerir_stop()">
                  <td width="35%">
                      <div class="one">
                          <!-- <div class="two" id='sim2real'><img src='images/sim2real.png' alt="sym" width="80%"
                                                            style="border-style: none"></div> -->
                          <!-- center the image -->
                          <div class="two" id="activerir" style="display: flex; justify-content: center; align-items: center; opacity: 0.9;">
                              <img src="images/active_rir.png" alt="sym" width="80%" style="border-style: none">
                          </div>
                          <script type="text/javascript">
                              function sim2real_start() {
                                  document.getElementById('activerir').style.opacity = "0.9";
                              }

                              function sim2real_stop() {
                                  document.getElementById('activerir').style.opacity = "1";
                              }

                              friendly_stop()
                          </script>
                  </div></td>
                  <td valign="top" width="65%">
                      <p><a href="">
                          <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                          <heading>ActiveRIR: Active Audio-Visual Exploration for Acoustic Environment Modeling</heading>
                      </a><br>Arjun Somayazulu, Sagnik Majumder, <strong>Changan Chen<strong>, Kristen Grauman<br>
                          <em>IROS 2024<br></em>
                          <a href="https://arxiv.org/abs/2404.16216">paper |</a>
                          <a href="https://vision.cs.utexas.edu/projects/active_rir">project</a>
                  </strong></strong></p></td>
              </tr>


              <tr onmouseout="soundingactions_start()" onmouseover="soundingactions_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="soundingactions"><img src="images/soundingactions.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function soundingactions_start() {
                              document.getElementById('soundingactions').style.opacity = "0.9";
                          }

                          function soundingactions_stop() {
                              document.getElementById('soundingactions').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="">
                          <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                          <heading>SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos</heading>
                      </a><br><strong>Changan Chen</strong>, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen Grauman<br>
                          <em>CVPR 2024<br></em>
                          <a href="https://arxiv.org/abs/2404.05206">paper |</a>
                          <a href="https://vision.cs.utexas.edu/projects/soundingactions/">project</a>
                  </p></td>
              </tr>

              <tr onmouseout="egoexo_start()" onmouseover="egoexo_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="egoexo"><img src="images/egoexo.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function egoexo_start() {
                              document.getElementById('egoexo').style.opacity = "0.9";
                          }

                          function egoexo_stop() {
                              document.getElementById('egoexo').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="">
                          <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                          <heading>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</heading>
                          <!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces" data-icon="octicon-star"-->
                          <!--                               data-show-count="true">Star</a>-->
                      </a><br>Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, ..., <strong>Changan Chen</strong>, ...,
                          Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella,
                          Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park,
                          James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray<br>
                          <em>CVPR 2024 <span style="color:red;">(<b>Oral</b>)</span><br></em>
                          <a href="https://ego-exo4d-data.org/">website |</a>
                          <a href="https://arxiv.org/abs/2311.18259">paper |</a>
                          <a href="https://www.youtube.com/watch?v=GdooXEBAnI8&amp;feature=youtu.be">video</a>
                  </p></td>
              </tr>

              <tr onmouseout="lemara_start()" onmouseover="lemara_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="lemara"><img src="images/lemara.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function lemara_start() {
                              document.getElementById('lemara').style.opacity = "0.9";
                          }

                          function lemara_stop() {
                              document.getElementById('lemara').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>Self-Supervised Visual Acoustic Matching</heading>
                          <!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces" data-icon="octicon-star"-->
                          <!--                               data-show-count="true">Star</a>-->
                      </a><br>Arjun Somayazulu, <strong>Changan Chen</strong>, Kristen Grauman<br>
                          <em>NeurIPS 2023<br></em>
                          <a href="https://arxiv.org/abs/2307.15064">paper |</a>
                          <a href="https://vision.cs.utexas.edu/projects/ss_vam/">project</a>
<!--                            <a href="https://github.com/facebookresearch/replay_dataset">data</a>-->
                  </p></td>
              </tr>

              <tr onmouseout="replay_start()" onmouseover="replay_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="replay"><img src="images/replay.jpeg" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function replay_start() {
                              document.getElementById('replay').style.opacity = "0.9";
                          }

                          function replay_stop() {
                              document.getElementById('replay').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://replay-dataset.github.io">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>Replay: Multi-modal Multi-view Acted Videos for Casual Holography</heading>
                          <!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces" data-icon="octicon-star"-->
                          <!--                               data-show-count="true">Star</a>-->
                      </a><br>Roman Shapovalov*, Yanir Kleiman*, Ignacio Rocco*, David Novotny, Andrea Vedaldi, <strong>Changan Chen</strong>, Filippos Kokkinos, Ben Graham, Natalia Neverova<br>
                          <em>ICCV 2023<br></em>
                          <a href="https://arxiv.org/abs/2307.12067">paper</a> |
                          <a href="https://replay-dataset.github.io/">project</a> |
                          <a href="https://github.com/facebookresearch/replay_dataset">data</a>
                  </p></td>
              </tr>
              <tr onmouseout="nvas_start()" onmouseover="nvas_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="nvas"><img src="images/nvas.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function nvas_start() {
                              document.getElementById('nvas').style.opacity = "0.9";
                          }

                          function nvas_stop() {
                              document.getElementById('nvas').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://vision.cs.utexas.edu/projects/nvas/">
                          <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                          <heading>Novel-View Acoustic Synthesis</heading>
                          <!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces" data-icon="octicon-star"-->
                          <!--                               data-show-count="true">Star</a>-->
                      </a><br><strong>Changan Chen</strong>, Alexander Richard, Roman Shapovalov, Vamsi Krishna
                          Ithapu, Natalia Neverova, Kristen Grauman, Andrea Vedaldi<br>
                          <em>CVPR 2023<br></em>
                          <a href="https://arxiv.org/abs/2301.08730">paper</a> |
                          <a href="https://vision.cs.utexas.edu/projects/nvas/">project</a> |
                          <a href="https://github.com/facebookresearch/novel-view-acoustic-synthesis">code</a> |
                          <a href="https://replay-dataset.github.io">data</a>
                  </p></td>
              </tr>

              </tbody></table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody><tr onmouseout="vida_start()" onmouseover="vida_stop()">
                      <td width="35%">
                          <div class="one">
                              <div class="two" id="vida"><img src="images/vida.png" alt="sym" width="100%" style="border-style: none"></div>
                          </div>
                          <script type="text/javascript">
                              function vida_start() {
                                  document.getElementById('vida').style.opacity = "0.9";
                              }

                              function vida_stop() {
                                  document.getElementById('vida').style.opacity = "1";
                              }

                              friendly_stop()
                          </script>
                      </td>
                      <td valign="top" width="65%">
                          <p><a href="http://vision.cs.utexas.edu/projects/learning-audio-visual-dereverberation">
                              <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                              <heading>Learning Audio-Visual Dereverberation</heading>
                              <!--                            <a class="github-button" href="https://github.com/facebookresearch/learning-audio-visual-dereverberation" data-icon="octicon-star"-->
                              <!--                               data-show-count="true">Star</a>-->
                          </a><br>
                              <strong>Changan Chen</strong>, Wei Sun, David Harwath, Kristen Grauman<br>
                              <em>ICASSP 2023<br></em>
                              <a href="https://arxiv.org/pdf/2106.07732.pdf">paper</a> |
                              <a href="http://vision.cs.utexas.edu/projects/learning-audio-visual-dereverberation">project</a> |
                              <a href="https://github.com/facebookresearch/learning-audio-visual-dereverberation">code</a>
                      </p></td>
                  </tr>
          </tbody></table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody><tr onmouseout="retrospective_start()" onmouseover="retrospective_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="retrospective"><img src="images/retrospective.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function vam_start() {
                              document.getElementById('retrospective').style.opacity = "0.9";
                          }

                          function vam_stop() {
                              document.getElementById('retrospective').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://arxiv.org/abs/2210.06849">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>Retrospectives on the Embodied AI Workshop</heading>
                          <!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces" data-icon="octicon-star"-->
                          <!--                               data-show-count="true">Star</a>-->
                      </a><br>Matt Deitke, Dhruv Batra, Yonatan Bisk, Tommaso Campari, Angel X. Chang, Devendra Singh Chaplot,
                          <strong>Changan Chen</strong>, Claudia Pérez D'Arpino, Kiana Ehsani, Ali Farhadi, Li Fei-Fei, Anthony Francis,
                          Chuang Gan, Kristen Grauman, David Hall, Winson Han, Unnat Jain, Aniruddha Kembhavi, Jacob Krantz,
                          Stefan Lee, Chengshu Li, Sagnik Majumder, Oleksandr Maksymets, Roberto Martín-Martín,
                          Roozbeh Mottaghi, Sonia Raychaudhuri, Mike Roberts, Silvio Savarese, Manolis Savva,
                          Mohit Shridhar, Niko Sünderhauf, Andrew Szot, Ben Talbot, Joshua B. Tenenbaum, Jesse Thomason,
                          Alexander Toshev, Joanne Truong, Luca Weihs, Jiajun Wu<br>
                          <em>arXiv 2022<br></em>
                          <a href="https://arxiv.org/abs/2210.06849">paper</a> |
                          <a href="https://embodied-ai.org/">website</a>
                  </p></td>
              </tr>

              <tr onmouseout="ss2_start()" onmouseover="ss2_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="ss2"><img src="images/ss2.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function vam_start() {
                              document.getElementById('ss2').style.opacity = "0.9";
                          }

                          function vam_stop() {
                              document.getElementById('ss2').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://soundspaces.org/">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning</heading>
<!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces" data-icon="octicon-star"-->
<!--                               data-show-count="true">Star</a>-->
                      </a><br>
                          <strong>Changan Chen*</strong>, Carl Schissler*, Sanchit Garg*, Philip Kobernik, Alexander Clegg, Paul Calamia, Dhruv Batra, Philip W Robinson, Kristen Grauman<br>
                          <em>NeurIPS 2022<br></em>
                          <em><span style="color:red;"><b>Distinguished Paper Award</b></span> at <a href="https://egovis.github.io/awards/2022_2023/">EgoVis Workshop, CVPR 2024</a><br></em>
                          <a href="https://arxiv.org/abs/2206.08312">paper</a> |
                          <a href="http://vision.cs.utexas.edu/projects/soundspaces2">project</a> |
                          <a href="https://soundspaces.org/">website</a> |
                          <a href="https://github.com/facebookresearch/sound-spaces">code</a>
                  </p></td>
              </tr>

              <tr onmouseout="fs_rir_start()" onmouseover="vam_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="fs_rir"><img src="images/fs_rir.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function vam_start() {
                              document.getElementById('fs_rir').style.opacity = "0.9";
                          }

                          function vam_stop() {
                              document.getElementById('fs_rir').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://vision.cs.utexas.edu/projects/fs_rir/">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>Few-Shot Audio-Visual Learning of Environment Acoustics</heading>
                      </a><br>
                          Sagnik Majumder, <strong>Changan Chen*</strong>, Ziad Al-Halah*, Kristen Grauman<br>
                          <em>NeurIPS 2022 <br></em>
                          <a href="https://arxiv.org/pdf/2206.04006.pdf">paper</a> |
                          <a href="https://vision.cs.utexas.edu/projects/fs_rir/">project</a>
                  </p></td>
              </tr>


              <tr onmouseout="vam_start()" onmouseover="vam_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="vam"><img src="images/vam.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function vam_start() {
                              document.getElementById('vam').style.opacity = "0.9";
                          }

                          function vam_stop() {
                              document.getElementById('vam').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="http://vision.cs.utexas.edu/projects/visual-acoustic-matching">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>Visual Acoustic Matching</heading>
<!--                            <a class="github-button" href="https://github.com/facebookresearch/visual-acoustic-matching" data-icon="octicon-star"-->
<!--                               data-show-count="true">Star</a>-->
                      </a><br>
                          <strong>Changan Chen</strong>, Ruohan Gao, Paul Calamia, Kristen Grauman<br>
                          <em>CVPR 2022 <span style="color:red;">(<b>Oral</b>)</span><br></em>
                          <a href="https://arxiv.org/abs/2202.06875">paper</a> |
                          <a href="https://twitter.com/MetaAI/status/1542233461380943872">video</a> |
                          <a href="http://vision.cs.utexas.edu/projects/visual-acoustic-matching">project</a> |
                          <a href="https://github.com/facebookresearch/visual-acoustic-matching">code</a> <br>
                          <strong>Media coverage: </strong> <a href="https://www.axios.com/2022/06/24/meta-wants-the-metaverse-to-sound-more-like-the-real-world?utm_source=twitter&amp;utm_medium=social&amp;utm_campaign=editorial&amp;utm_content=technology-metaverse">
                              <img src="media/axios-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://www.digitalinformationworld.com/2022/06/meta-plans-on-taking-digital.html">
                              <img src="media/digital-information-world-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://www.engadget.com/metas-latest-auditory-ai-promise-a-more-immersive-ar-vr-experience-130029625.html">
                              <img src="media/Engadget-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://au.news.yahoo.com/metas-latest-auditory-ai-promise-a-more-immersive-ar-vr-experience-130029625.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly9sLndvcmtwbGFjZS5jb20v&amp;guce_referrer_sig=AQAAAKKTGvJL96cghtvp21ZK0OIQKsLo4t0UABuDrp_f2qXUqHpbyGq-833x6U92N4vYJmasC5DNthkY5Pkq-ipMrapT8xOhM8sxUplP9jCP98SLfREzv2yymSFEOHADjEVXTFmqVgKzFcL61OqKVdtrXTtIVpe9OaCBBQbOyb13lCb_">
                              <img src="media/yahoo_news.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a> <br>
                          <a href="https://musically.com/2022/06/27/mark-zuckerberg-on-ar-vr-getting-spatial-audio-right-is-key/">
                              <img src="media/musically.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://techmonitor.ai/technology/emerging-technology/meta-audio-ai-metaverse">
                              <img src="media/tech-monitor.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://www.techradar.com/news/meta-wants-the-virtual-landscape-to-sound-like-real-life">
                              <img src="media/TechRadar_logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://siliconangle.com/2022/06/24/meta-building-better-ai-driven-audio-virtual-reality/">
                              <img src="media/sa-press-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://www.socialmediatoday.com/news/metas-developing-new-spatial-audio-tools-for-ar-and-vr-to-enhance-virtual/626071/?utm_source=dlvr.it&amp;utm_medium=twitter">
                              <img src="media/smt.svg" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://www.zacks.com/stock/news/1944437/meta-platforms-meta-develops-3-new-ai-models-for-metaverse">
                              <img src="media/zacks.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://www.nasdaq.com/articles/meta-platforms-meta-develops-3-new-ai-models-for-metaverse">
                              <img src="media/NASDAQ_Logo.svg.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="http://www.aitimes.com/news/articleView.html?idxno=146066">
                              <img src="media/aitimes.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                  </p></td>
              </tr>
              </tbody></table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody><tr onmouseout="savvn_start()" onmouseover="savvn_stop()">
                      <td width="35%">
                          <div class="one">
                              <div class="two" id="savvn"><img src="images/savvn.png" alt="sym" width="100%" style="border-style: none"></div>
                          </div>
                          <script type="text/javascript">
                              function savvn_start() {
                                  document.getElementById('vida').style.opacity = "0.9";
                              }

                              function savvn_stop() {
                                  document.getElementById('vida').style.opacity = "1";
                              }

                              friendly_stop()
                          </script>
                      </td>
                      <td valign="top" width="65%">
                          <p><a href="https://yyf17.github.io/SAAVN/">
<!--                                <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                              <heading>Sound Adversarial Audio-Visual Navigation</heading>
<!--                                <a class="github-button" href="https://github.com/yyf17/SAAVN/" data-icon="octicon-star"-->
<!--                                   data-show-count="true">Star</a>-->
                          </a><br>Yinfeng Yu, Wenbing Huang, Fuchun Sun, <strong>Changan Chen</strong>, Yikai Wang, Xiaohong Liu <br>
                              <em>ICLR 2022<br></em>
                              <a href="https://openreview.net/forum?id=NkZq4OEYN-">paper</a> |
                              <a href="https://yyf17.github.io/SAAVN/">project</a> |
                              <a href="https://github.com/yyf17/SAAVN/tree/main">code</a>
                      </p></td>
                  </tr>
          </tbody></table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody><tr onmouseout="savi_start()" onmouseover="savi_stop()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="savi"><img src="images/savi.png" alt="sym" width="90%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function savi_start() {
                              document.getElementById('savi').style.opacity = "0.9";
                          }

                          function savi_stop() {
                              document.getElementById('savi').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="http://vision.cs.utexas.edu/projects/semantic_audio_visual_navigation">
                          <heading>Semantic Audio-Visual Navigation</heading>
<!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces" data-icon="octicon-star"-->
<!--                               data-show-count="true">Star</a>-->
                      </a><br>
                          <strong>Changan Chen</strong>, Ziad Al-Halah, Kristen Grauman<br>
                          <em>CVPR 2021<br></em>
                          <a href="https://arxiv.org/pdf/2012.11583.pdf">paper</a> |
                          <a href="http://vision.cs.utexas.edu/projects/semantic_audio_visual_navigation">project</a> |
                          <a href="https://github.com/facebookresearch/sound-spaces/tree/master/ss_baselines/savi">code</a>
                          <!--<a href="https://github.com/Deanplayerljx/tab-vcr/">code</a>-->
                  </p></td>
              </tr>

          </tbody></table><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody><tr onmouseout="av_wan_stop()" onmouseover="av_wan_start()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="av_wan"><img src="images/av_wan.jpg" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function av_wan_start() {
                              document.getElementById('av_wan').style.opacity = "0.9";
                          }

                          function av_wan_stop() {
                              document.getElementById('av_wan').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">
                          <heading>Learning to Set Waypoints for Audio-Visual Navigation</heading>
<!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces" data-icon="octicon-star"-->
<!--                               data-show-count="true">Star</a>-->
                      </a><br>
                          <strong>Changan Chen</strong>, Sagnik Majumder, Ziad Al-Halah, Ruohan Gao,
                          Santhosh K. Ramakrishnan, Kristen Grauman<br>
                          <em>ICLR 2021<br></em>
                          <a href="https://arxiv.org/pdf/2008.09622.pdf">paper</a> |
                          <a href="http://vision.cs.utexas.edu/projects/audio_visual_waypoints">project</a> |
                          <a href="https://github.com/facebookresearch/sound-spaces/tree/master/ss_baselines/av_wan">code</a>
                          <!--<a href="https://github.com/Deanplayerljx/tab-vcr/">code</a>-->
                  </p></td>
              </tr>

              <tr onmouseout="visualecho_stop()" onmouseover="visualecho_start()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="visualecho"><img src="images/visual_echoes.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function visualecho_start() {
                              document.getElementById('visualecho').style.opacity = "0.9";
                          }

                          function visualecho_stop() {
                              document.getElementById('visualecho').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://vision.cs.utexas.edu/projects/visualEchoes/">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>VisualEchoes: Spatial Image Representation Learning through Echolocation</heading>
<!--                            <a class="github-button" href="https://github.com/facebookresearch/VisualEchoes"-->
<!--                               data-icon="octicon-star" data-show-count="true">Star</a>-->
                      </a><br>
                          Ruohan Gao, <strong>Changan Chen</strong>, Carl Schissler,
                          Ziad Al-Halah, Kristen Grauman<br>
                          <em>ECCV 2020<br></em>
                          <a href="https://arxiv.org/pdf/2005.01616.pdf">paper</a> |
                          <a href="http://vision.cs.utexas.edu/projects/visualEchoes/">project</a> |
                          <a href="https://github.com/facebookresearch/VisualEchoes">code</a>
                  </p></td>
              </tr>

              <tr onmouseout="avnav_stop()" onmouseover="avnav_start()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="avnav"><img src="images/avnav.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function avnav_start() {
                              document.getElementById('avnav').style.opacity = "0.9";
                          }

                          function avnav_stop() {
                              document.getElementById('avnav').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://soundspaces.org/">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>SoundSpaces: Audio-Visual Navigation in 3D Environments</heading>
<!--                            <a class="github-button" href="https://github.com/facebookresearch/sound-spaces"-->
<!--                               data-icon="octicon-star" data-show-count="true">Star</a>-->
                      </a><br>
                          <strong>Changan Chen*</strong>, Unnat Jain*, Carl Schissler, Sebastia Vicenc Amengual Gari,
                          Ziad Al-Halah, Vamsi Krishna Ithapu, Philip Robinson, Kristen Grauman<br>
                          <em>ECCV 2020 <span style="color:red;">(<b>Spotlight</b>)</span><br></em>
                          <a href="https://arxiv.org/pdf/1912.11474.pdf">paper</a> |
                          <a href="http://vision.cs.utexas.edu/projects/audio_visual_navigation/">project</a> |
                          <a href="https://github.com/facebookresearch/sound-spaces">code</a> |
                          <a href="https://soundspaces.org/">website</a> <br>
                          <strong>Media coverage: </strong> <a href="https://www.technologyreview.com/2020/08/21/1007523/facebook-ai-robot-assistants-hear-and-see">
                              <img src="media/fb-ai-blog-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://www.technologyreview.com/2020/08/21/1007523/facebook-ai-robot-assistants-hear-and-see">
                              <img src="media/mit-tech-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a> <br>
                          <a href="https://siliconangle.com/2020/08/21/facebook-open-sources-embodied-ai-tools-advance-robotic-navigation/">
                              <img src="media/sa-press-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://venturebeat.com/2020/08/21/facebook-releases-tools-to-help-ai-navigate-complex-environments/">
                              <img src="media/venture-beat-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                          <a href="https://www.zdnet.com/article/facebook-is-building-robots-to-help-you-find-your-ringing-phone/">
                              <img src="media/zdnet-press-logo.png" alt="media logo" height="18" style="border:1px solid #D3D3D3;" hspace="0"></a>
                  </p></td>
              </tr>

              <tr onmouseout="rgl_stop()" onmouseover="rgl_start()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="rgl"><img src="images/rgl.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function rgl_start() {
                              document.getElementById('rgl').style.opacity = "0.9";
                          }

                          function rgl_stop() {
                              document.getElementById('rgl').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://github.com/ChanganVR/RelationalGraphLearning">
<!--                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">-->
                          <heading>Relational Graph Learning for Crowd Navigation</heading>
<!--                            <a class="github-button" href="https://github.com/ChanganVR/RelationalGraphLearning"-->
<!--                               data-icon="octicon-star" data-show-count="true">Star</a>-->
                      </a><br>
                          <strong>Changan Chen*</strong>, Sha Hu*, Payam Nikdel, Greg Mori, Manolis Savva<br>
                          <em>IROS 2020<br></em>
                          <a href="https://arxiv.org/abs/1909.13165">paper</a> |
                          <a href="https://github.com/ChanganVR/RelationalGraphLearning">code</a>
                  </p></td>
              </tr>

              <tr onmouseout="crowdnav_stop()" onmouseover="crowdnav_start()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="crowdnav"><img src="images/crowdnav.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function crowdnav_start() {
                              document.getElementById('crowdnav').style.opacity = "0.9";
                          }

                          function crowdnav_stop() {
                              document.getElementById('crowdnav').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://github.com/vita-epfl/CrowdNav/">
                          <heading>Crowd-Robot Interaction: Crowd-aware Robot Navigation with Attention-based Deep
                              Reinforcement Learning
                          </heading>
<!--                            <a class="github-button" href="https://github.com/vita-epfl/CrowdNav"-->
<!--                               data-icon="octicon-star" data-show-count="true">Star</a>-->
                      </a><br>
                          <strong>Changan Chen</strong>, Yuejiang Liu, Sven Kreiss, Alexandre Alahi<br>
                          <em>ICRA 2019<br></em>
                          <a href="https://arxiv.org/pdf/1809.08835.pdf">paper</a> |
                          <a href="https://github.com/vita-epfl/CrowdNav/">code</a>
                  </p></td>
              </tr>

              <tr onmouseout="compression_stop()" onmouseover="compression_start()">
                  <td width="35%">
                      <div class="one">
                          <div class="two" id="compression"><img src="images/compression.png" alt="sym" width="100%" style="border-style: none"></div>
                      </div>
                      <script type="text/javascript">
                          function compression_start() {
                              document.getElementById('compression').style.opacity = "0.9";
                          }

                          function compression_stop() {
                              document.getElementById('compression').style.opacity = "1";
                          }

                          friendly_stop()
                      </script>
                  </td>
                  <td valign="top" width="65%">
                      <p><a href="https://github.com/ChanganVR/ConstraintAwareCompression">
                          <heading>Constraint-Aware Deep Neural Network Compression</heading>
<!--                            <a class="github-button" href="https://github.com/ChanganVR/ConstraintAwareCompression"-->
<!--                               data-icon="octicon-star" data-show-count="true">Star</a>-->
                      </a><br>
                          <strong>Changan Chen</strong>, Frederick Tung, Naveen Vedula, and Greg Mori<br>
                          <em>ECCV 2018<br></em>
                          <a href="files/constraint_aware_compression.pdf">paper</a> |
                          <a href="https://github.com/ChanganVR/ConstraintAwareCompression">code</a>
                  </p></td>
              </tr>
          </tbody></table>

              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody><tr>
                      <td>
                          <heading>Invited Talks</heading>
                      </td>
                  </tr>
              </tbody></table>
              <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
                  <colgroup>
                      <col width="15%">
                      <col width="85%">
                  </colgroup>
                  <tbody>
                  <tr>
                      <td valign="top" align="center"><strong>Dec 2023</strong></td>
                      <td> Invited talk at <a href="https://steinhardt.nyu.edu/marl">NYU</a>, "4D Audio-Visual Perception: Simulating, Synthesizing and Navigating with Sounds in Spaces" (<a href="talks/marl.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>June 2023</strong></td>
                      <td> Keynote talk at <a href="https://sites.google.com/view/perdream/home">PerDream Workshop</a>, ICCV 2023, "Audio-Visual Embodied AI: From Simulating to Navigating with Sounds in Spaces" (<a href="talks/perdream%20october%2003.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>June 2023</strong></td>
                      <td> Keynote talk at <a href="https://sightsound.org">Sight and Sound Workshop</a>, CVPR 2023, "Novel-view Acoustic Synthesis" (<a href="talks/sight_and_sound.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>June 2023</strong></td>
                      <td> Keynote talk at <a href="https://sites.google.com/view/ambientaiicassp2023/home">Ambient AI Workshop</a>, ICASSP 2023, "Visual-acoustic Learning" (<a href="talks/ambient_AI.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>Feb 2023</strong></td>
                      <td> Invited talk at <a href="https://www.texasacoustics.org/current-seminar">Texas Acoustics</a>, "Visual Learning of Sound in Spaces" (<a href="talks/MIT%20Jan%2018.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>Jan 2023</strong></td>
                      <td> Invited talk at <a href="https://cdfg.mit.edu/">MIT</a>, "Visual Learning of Sound in Spaces" (<a href="talks/MIT%20Jan%2018.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>Nov 2022</strong></td>
                      <td> Invited talk at <a href="https://ai.facebook.com/">FAIR, Meta AI</a>, "Visual Learning of Sound in Spaces" (<a href="talks/MIT%20Jan%2018.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>June 2022</strong></td>
                      <td> Oral talk at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>, "Visual Acoustic Matching" (<a href="talks/cvpr_presentation.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>June 2021</strong></td>
                      <td> Invited talk at <a href="https://tech.fb.com/ar-vr/">Facebook Reality Labs</a>, "Learning Audio-Visual Dereverberation" (<a href="talks/vida_frl.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>June 2021</strong></td>
                      <td> Invited talk at <a href="https://eyewear-computing.org/EPIC_CVPR21/program">EPIC Workshop</a>, CVPR 2021, "Semantic Audio-Visual Navigation" (<a href="talks/savi_epic.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>Sept. 2020</strong></td>
                      <td> Invited talk at <a href="https://www.cs.utexas.edu/~yukez/cs391r_fall2020">CS391R: Robot Learning</a> at UT Austin, "<i>"Audio-Visual Navigation</i>" (<a href="talks/robot_learning.pdf">Slides</a>)
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>Dec. 2018</strong></td>
                      <td> Invited talk at <a href="https://www.meetup.com/SwissAI">SwissAI Meetup</a>, "Navigation in Crowds: From 2D Navigation to Visual Navigation"
                      </td>
                  </tr>
                  <tr>
                      <td valign="top" align="center"><strong>Nov. 2018</strong></td>
                      <td> Invited talk at <a href="https://www.idiap.ch/webarchives/sites/www.idiap.ch/workshop/smld2018/#Crowd_Robot_Interaction__Crowd_a">Swiss Machine Learning Day</a>, "Crowd-aware Robot Navigation with Attention-based DRL"
                      </td>
                  </tr>
<!--                    <tr>-->
<!--                        <td valign="top" align="center"><strong>May 2018</strong></td>-->
<!--                        <td> Poster presentation at <a href="http://www.en.cs.zju.edu.cn/2018/0604/c55704a2239311/page.htm">SFU-ZJU Joint Symposium</a>, "Constraint-aware Deep Neural Network Compression"-->
<!--                        </td>-->
<!--                    </tr>-->
                  </tbody>
              </table>


              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody>
                  <tr>
                      <td>
                          <heading>Affiliations</heading>
                      </td>
                  </tr>
                  </tbody>
              </table>
              <table align="center">
                  <tbody>
                  <tr>
                      <td width="16%" align="center">
                          <a href="https://www.zju.edu.cn/english/" target="_blank">
                              <img style="width:120px" src="images/zju.png"></a>&nbsp;
                      </td>
                      <td width="16%" align="center">
                          <a href="https://www.sfu.ca/" target="_blank">
                              <img style="width:120px" src="images/sfu.png"></a>&nbsp;
                      </td>
                      <td width="16%" align="center">
                          <a href="https://www.epfl.ch/en/" target="_blank">
                              <img style="width:120px" src="images/epfl.png"></a>&nbsp;
                      </td>
                      <td width="16%" align="center">
                          <a href="https://research.fb.com/category/facebook-ai-research" target="_blank">
                              <img style="width:120px" src="images/fair.png"></a>&nbsp;
                      </td>
                      <td width="16%" align="center">
                          <a href="https://www.utexas.edu/" target="_blank">
                              <img style="width:120px" src="images/ut.png"></a>&nbsp;
                      </td>
                      <td width="16%" align="center">
                          <a href="https://www.utexas.edu/" target="_blank">
                              <img style="width:120px" src="images/stanford.png"></a>&nbsp;
                      </td>
                  </tr>
                  <tr>
                      <td width="20%" align="center"><font size="2">ZJU, China<br>2014-2016</font></td>
                      <td width="20%" align="center"><font size="2">SFU, Canada<br>2016-2019</font></td>
                      <td width="20%" align="center"><font size="2">EPFL, Switzerland<br>2018</font></td>
                      <td width="20%" align="center"><font size="2">FAIR, USA &amp; UK<br>2020 - 2022</font></td>
                      <td width="20%" align="center"><font size="2">UT AUSTIN, USA<br>2019 - 2024</font></td>
                      <td width="20%" align="center"><font size="2">Stanford, USA<br>2024 - present</font></td>
                  </tr>
                  </tbody>
              </table>

          <!--<table id="thanks" width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
          <!--<tr>-->
          <!--<td>-->
          <!--<br>-->
          <!--<p align="right"><font size="2">-->
          <!--<a href="http://www.cs.berkeley.edu/~barron/">(imitation is the sincerest form of flattery)</a>-->
          <!--</font>-->
          <!--</p>-->
          <!--</td>-->
          <!--</tr>-->
          <!--</table>-->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody>
              <tr>
                  <td><br>
                      <p align="right"><font size="2">
                          Template credits: <a href="https://unnat.github.io/">Unnat</a>, <a href="https://jonbarron.info/">Jon</a>
                      </font></p></td>
              </tr>
              </tbody>
          </table>

      </td>
  </tr>
</tbody></table>


<div id="sff-extension" data-sff-extension-version="0.6.0.0"></div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>